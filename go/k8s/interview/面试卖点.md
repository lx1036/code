

# 与k8s api-server通信的list-watch机制和informer模块的工作原理?
**灵感来自 https://zhuanlan.zhihu.com/p/59660536**
其实就是消息生产者和消费者模型：
(1)生产者ListAndWatch api-server: Reflector对象作为生产者，首先会List()特定资源，然后会Watch()该特定的资源，比如pod资源，这样对于以后每次pod资源的任何事件，比如
CreateEvent/UpdateEvent/DeleteEvent，这些称为ResourceEvent对象如{type: "Added", object: &Pod{},}，Reflector会把这些ResourceEvent对象
存入Store对象中。这里，Reflector对象作为生成者，Store对象作为消息队列，存储ResourceEvent对象。
这里Watch()特定资源意思是：api-server会主动把每次ResourceEvent发给客户端，这里使用HTTP/1.1长链接并分块传输编码实现的，response headers里包含 Transfer-Encoding: chunked，这样
客户端和api-server保持长链接，api-server只要有pod资源的ResourceEvent发生，就会持续不断分块传输ResourceEvent。实现了服务端主动push数据给客户端。
(2)消息队列: DeltaFIFO Queue作为消息队列数据结构，就是一个很关键的Store对象，且是并发安全的一个queue，来存储ResourceEvent。
(3)消费者：Controller对象，作为消费者使用DeltaFIFO.Pop()来消费ResourceEvent，获取该ResourceEvent对象。
(4)Informer: 总体封装对象，包含Store和Controller对象，以及最重要的ResourceEventHandler对象，这是一个interface，
该interface包含的onAdd()/onUpdate()/onDelete()操作方法，具体操作由开发者去定义，且用户定义的都是写操作，比如：
```go
import (
 "k8s.io/client-go/tools/cache"
)
podGenericInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
    AddFunc: func(obj interface{}) {

    },
    UpdateFunc: func(old, new interface{}) {
    
    },
    DeleteFunc: func(obj interface{}) {
    
    },
})
```
(5)读操作：同时Informer包含一个Indexer对象，即Store对象，也就是threadSafeMap对象，在controller作为消费者去DeltaFIFO queue当中取ResourceEvent时，
同时也会将该对象存入threadSafeMap对象中，这样读操作会直接从threadSafeMap对象中去取。

# 使用controller-runtime pkg写operator以及controller-runtime工作原理?




# k8s负载均衡几种形式和工作原理?
k8s负载均衡包括四层service和七层ingress:
service类型包含ClusterIP、NodePort和LoadBalancer:
(1)k8s主要使用kube-proxy组件作为DaemonSet部署在每一个node上，根据启动参数mode不同，要么使用iptables模式或者ipvs模式，来在内核层设置流量包的路由规则。
kube-proxy组件会watch api-server上的service和endpoint资源对象，当创建一个service后，每一个kube-proxy会调用iptables或ipvs客户端来创建路由规则，
主要是nat表内的规则，会在kube-services chain上创建路由规则，tcp流量根据cluster ip一步步跳转，再跳转到kube-sep chain上，最后到对应的pod ip，然后从该节点出去，下一跳是pod ip。其中，ipvs作用在
input chain，而不是像iptables作用在prerouting chain，所以ipvs模式需要给该vip在本机设置个虚拟网卡。
而且，与iptables作为防火墙功能不一样，ipvs本身就是作为负载均衡功能，支持多达八种左右负载均衡算法，如最小链接、轮询、权重等等。
且iptables使用链表存储路由规则，ipvs使用哈希表存储，这样流量包在查找下一跳规则时效率更高，减少流量包延迟。
(2)流量包出node下一跳是pod ip，如果使用calico网络插件，则使用的是路由转发，会找到路由规则即pod ip在这个IP段内请跳转到该node ip上，这些路由规则动态更新是calico felix做的，
路由规则广播是calico bird做的。从而实现了跨node的pod之间相互通信。
(3)流量暴露三种方式：
带有ExternalIPs ClusterIP Service，边缘节点形式，选择几个node节点作为边缘节点，然后再用个vip下rs挂载这些node节点，业务直接vip:port对外暴露；
NodePort Service，port成为node节点的port，类似docker里host network形式，这样每一个node都会开这个port，没有边缘节点形式友好，只会有几个node开port；
自研LoadBalancer直连Pod IP(lvs vip的rs为pod ip)，使用kubebuilder工具自定义crd，编写cr设置业务的deployment/service，然后watch deployment，只要该
deployment在你的cr里，就通过service获取endpoint对象，进而获取所有pod ips，然后调用lvs api获取该vip下所有的pod ips，进行diff，有修改则调用add/delete lvs api去更新
lvs下的pod ips，整个过程也就是reconcile，最终达到lvs vip下pod ips期望值和实际业务pod ips保持同步；

nginx-ingress:
(1)使用了nginx反向代理到upstream模块实现七层的负载均衡，upstream指向的是service name，具有服务发现功能。
(2)流量暴露：
nginx-ingress部署在指定node节点上，并挂载在某vip下，业务可以根据host域名来反向代理到不同的upstream上。
ingress里定义的每一个业务在nginx.conf里就是一个server模块，path是server下的location模块。可以通过docker cp命令把容器内的nginx.conf拷贝出来分析分析。



# etcd基本架构以及内部设计和原理?



