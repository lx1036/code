# 个人介绍
* 姓名：刘祥
* 性别：男
* 出生日期：1991-10-10
* 学位：北京理工大学学士(2008-09 ~ 2012-07)/北京航空航天大学硕士(2012-09 ~ 2015-07)
* 工作经验：7 年
* 毕业时间：2015-07-01
* 联系电话/微信号：13426116367
* 电子邮箱：lx1036@126.com或lx20081036@gmail.com
* 目前职位：奇虎360容器云技术专家
* 技术专栏：https://juejin.cn/user/800100194726088/posts
* 应聘职位：云原生研发工程师(Kubernetes方向)
* K8S证书：CKA 证书

# 工作经历及项目经验

## 北京当当网信息技术有限公司(2015-07 ~ 2016-07)
主要使用PHP语言重构一些老业务代码和迭代业务新功能，主要工作内容包括：
(1)负责当当图书和店铺域的改版和优化，并负责后续版本迭代工作。
(2)负责当当优品馆全面改版项目，对一些老代码进行了重构优化，提高代码可读性。

## RightCapital(2016-07 ~ 2019-07)
参与创业，加入时公司共5个人，北京和纽约办公室各2-3个人。作为全栈工程师参与创业，写后端和前端业务。
主要是用 PHP 语言和 Laravel Web 框架做一款金融软件，面向美国市场。主要工作内容包括：
(1)使用 PHP 框架 Laravel 编写金融软件 RightCapital 后端的 Restful API，并使用 PHPUnit/Mockery 编写单元测试和集成测试。
同时，结合业务需求，对 Laravel 做了很多二次开发，并做成共享私有包，并编写 API 的 Swagger 文档。

(2)使用 Angular 作为前端，Laravel 作为后端，并使用 Ant Design 组件库编写 Admin 后台，供美国客服团队使用。
重写金融软件 RightCapital 前端模块，把其从 Angular.js 重写升级到 Angular 框架。

(3)运维云服务器AWS，搭建一些DevOps软件工具，如Gitlab CI/CD、编写Docker images等等，并使用Terraform/Ansible开发一些提高工作效率的工具等等。

## 奇虎360(2019-08 ~ 至今)
在360技术中台云平台部门负责全公司的 K8S 集群维护，主要工作内容包括：

### (1) 在离线混合部署项目开发
(7.1) 基于阿里混部内核的 POC 验证测试

(7.2) pod 调度器 plugin 开发
对


相关经验：对容器网络、存储和调度有不错的了解和实践，对在离线混合部署和大数据上 K8sS 有一定的实践经验。


### (2) 负载均衡项目 LoadBalancer Operator(依赖外部 LVS)，实现暴露 K8S 集群内 Pod 为集群外提供服务
内容：替换旧的通过 externalIP service 对外暴露服务方式(内部俗称边缘节点方式，与 NodePort 方式类似)，使用 K8S Pod 直连公司已有的生产 LVS (keepalived 搭建)集群方案，
实现集群外部流量由 LVS 集群接管。K8S 集群内部四层负载均衡由 kube-proxy 组件来接管(部分 K8S 由 Cilium eBPF 替换 kube-proxy)，外部由 Cilium+BGP(bird) 来宣告 Pod Cidr。
最终，实现 LVS VIP 直连 Pod IP，并结合 BGP 宣告分配给每一个 Node 的 Pod Cidr，使得 Pod IP 公司内网可达，实现包从 LVS 集群直接跳转 Pod 所在的 Node，减少网络跳转，提高网络性能，实现了服务对外暴露方式。
初始版本把 LVS 相关配置放入 ClusterIP Service Annotation 里，后续使用 LoadBalancer CRD 模式重构了一版本并生产可用，使得配置更简单，且更具有可观测性。

难点：动态感知 PodIP 变化并更新 LVS VIP 下的 RS 变化，使得 LVS 侧 RS 列表和 K8S 侧 Pod 列表一致；
使用 K8S ReadinessGate/Webhook 功能使得 Pod 滚动时只有 LVS 侧资源创建完成后，该新 Pod 才能加入 LVS 侧 RS 列表，才能对外提供服务；
使用 Webhook 实现 Pod graceful-shutdown，解决 Pod 资源销毁时，流量丢失问题。

现状：大多数生产 K8S 集群已经使用该方案作为暴露 Pod 服务的优先方案。

### (3) 负载均衡项目 LoadBalancer Operator(不依赖外部 LVS)，实现暴露 K8S 集群内 Pod 为集群外提供服务
背景：LVS 集群不稳定尤其性能原因(RS数量过大后 keepalived 配置更新容易失败导致应该删除的 RS 没有删除等问题，造成流量丢失，目前临时解决方案是 LVS 定期重启)，
以及 to B 交付时无法交付 LVS 套件(目前只有 NodePort 方案)，所以必须探索出一套不依赖于外部 LVS 的暴露服务的方案。
公司内使用作为 LoadBalancer Operator(依赖外部 LVS) 方案的补充，公司外交付时的另一种方案选项。

内容：替换 LoadBalancer Operator(依赖外部 LVS)方案，借助 K8S 集群内已经实现了四层负载均衡的机制，和 K8S LoadBalancer Service 机制，开发
K8S LoadBalancer Service IPAM Operator (Deployment 部署)来给 LoadBalancer Service 分配 ServiceIP，且支持多个 ServiceIPPool 以及指定特定的 ServiceIPPool。

同时为了使得该 ServiceIP 也在集群外可达，开发 LoadBalancer Service BGP Speaker 宣告 ServiceIP(Daemonset 部署)，且通过 BGPPeer CRD 配置，
使得 LoadBalancer ServiceIP 在集群外可达，可以不再依赖外部 LVS 集群，且网络跳转数量更少，性能更高。并且支持 Service ExternalTrafficPolicy 功能。

同时为了替换 K8S 集群内部署的 bird Daemonset 软件来宣告 Cilium Operator 给每一个 Node 分配的 Pod Cidr，因此开发了 Pod Cidr BGP Speaker 来宣告，使得 Pod IP 在集群外可达，目前已经在部分生产 K8S 集群上线。

该方案相比于 LoadBalancer Operator(依赖外部 LVS) 优点：不再依赖于外部 LVS 集群，交付更简单；网络跳转更少，性能更高；
Pod 滚动时为了减少流量丢失，依赖外部 LVS 方案使用了 ReadinessGate 等一系列缓解方案，而本方案则把 Pod 滚动交给了 kube-proxy 或 Cilium 去解决，不存在流量丢失问题。
该方案相比于 LoadBalancer Operator(依赖外部 LVS) 缺点：依赖 BGP 宣告路由，且每一个 ServiceIP 对应多个 Node 的路由，导致上层交换机路由数量会变大。

现状：公司内部作为 LoadBalancer Operator(依赖外部 LVS) 方案的补充使用，公司外部交付优先考虑的方案。

### (4) Cilium IPAM Operator 项目
内容：针对 K8S 集群内需要根据 nodeSelector 选择不同的 IPPool 需求，比如集群内有些 worker nodes 是专属某个业务的，且该业务因为调用外部服务需要使用另一个 pod cidr。
并且尽可能需要一个 Node 可以配置多个 PodCIDR，实现当 Node 的 IP 资源不足时，可以动态扩容 IP，该需求类似于 Calico 支持多个 IPPool 功能，且公司业务需要该功能。

难点：但是 Cilium 目前不内置支持根据 nodeSelector 选择不同的 IPPool 需求，且无法支持一个 Node 可以配置多个 PodCIDR，需要根据 Cilium IPAM 自定义开发。
目前已经根据两种不同的 Cilium IPAM 分别开发了对应的 Operator。

Cilium Kubernetes IPAM Operator: 根据 Cilium Kubernetes IPAM 机制开发自定义的 Operator，结合 cilium-agent 从 K8S Node 或 Node Annotation 中读取 PodCIDR 机制，所以选择
关闭 kube-controller-manager 给 K8S Node 分配 PodCIDR，开发自定义 Operator 来根据 Node Labels 选择不同的 IPPool 并分配对应的 PodCIDR，并添加到
Node Annotation 中，从而实现了不同 K8S Node 可以选择不同的 IPPool 需求。
目前已经生产可用，经过一段时间使用，符合业务需要，且不需要更改 cilium-agent 以及 BGP 相关的配置参数。

Cilium CRD IPAM Operator: 更进一步，第一种方案尽管已经满足业务需要，但是该方案不支持一个 Node 配置多个 PodCIDR，所以更进一步，选择 Cilium CRD IPAM 机制，再次开发自定义的 Operator，
支持按需动态扩容和回收节点的 PodCIDR IP 资源。该方案实现复杂，同时需要更改 cilium-agent 相关参数，比如 cilium-agent 默认只会为有且仅有一个 PodCidr 创建一个路由指向
网卡 cilium_host，为了支持多个 PodCIDR，需要开启每一个 Pod 一个路由的配置参数，而出于性能考虑这不是 Cilium 的默认行为，等等几个其他参数配置；同时还需要修改 BGP speaker 软件 bird 的配置。
总之，该方案有点 hack。总之，出于稳定性考虑以及业务需要考虑，暂时在测试 K8S 集群部署使用。

现状：出于稳定考虑，目前生产 K8S 先小批量使用 Cilium Kubernetes IPAM Operator，后续再考虑使用 Cilium CRD IPAM Operator。

### (5) 自定义容器网络插件 CNI 开发项目
内容：针对容器上虚机场景，使用 ipvlan 技术连通不同 namespace 的容器，开发 CNI。
同时使用 iptables/ipset/conntrack 技术基本实现了 NetworkPolicy 功能，包括：。


### (6) fusefs-csi 和 fusefs 项目

#### (6.1) 开发 fusefs 项目：fusefs 项目包含 fuse-client、master(raft) 和 meta-partition(multi-raft) 三个模块。
部署时 master 作为控制平面一般部署3节点，使用raft保证数据强一致性；meta-partition 节点可以无限扩展，每个 meta partition 默认3节点，使用 multi-raft 保证数据强一致性；
fuse-client 进程置于 fusefs-csi pod 里，每个 k8s worker node 上每个 pv(可以被多个 pod 挂载) 启动一个 fuse-client 进程来实现 fuse 挂载。 

* 主要参与开发 fuse-client 模块，使用 fuse 实现本地化直接读写文件，数据存储在远程 S3 上。inode 和 dentry 数据结构都缓存在本地内存中，
其中 inode 使用 LRU 数据结构存储，根据其有效时间来从 meta-partition 刷新 inode。并且修改第三方 fuse 包，使其支持 macOS 系统，使得可以 mac 本地运行。

* 主要参与开发 master 模块，主要提供 meta node 注册相关 api；create/delete volume 等相关 api 被 fusefs-csi 调用，
并在 volume 创建时根据 meta node 使用率选择对应数量的 meta node，在每一个 meta node 上创建包含 inode 范围的 meta partition 数据。
使用 raft 来实现数据强一致性，raft log 和相关 term 等配置数据，存储在 boltdb 中。通过定期 raft snapshot 实现状态机的快照，
状态机数据比如 volume 及其相关的 meta partition 等数据，也是持久化到 boltdb 中。所以，有两个 boltdb 文件，一个持久化 raft log 和 raft 配置数据，一个是持久化状态机数据。

* 部分参与 meta-partition 模块，该模块主要管理 inode/dentry 元数据 。每一组 meta partition 由 raft 保证强一致性，多组 meta partition 组成
raft group。对于每一组 raft meta partition，其状态机由两个 B-Tree 分别创建和删除 inode/dentry。

#### (6.2) 开发 fusefs-csi 项目：为团队自研的分布式文件存储实现一个 K8s CSI，方便业务 pod 动态挂载 fusefs pvc。
* 独立开发整个 CSI 项目，完美解决了社区 fuse 分布式文件系统共有难题：CSI Pod 重启，业务 Pod 挂载点损坏，导致业务 Pod 无法读写数据。解决方案为两期：
  * CSI Pod 重启后，需要加上 fuse recovery 机制，根据 K8s VolumeAttachment 对象获取所有相关数据，再在 CSI Pod 内重启 fusefs-client 进程。(已经回馈给京东 cubefs-csi 社区)
  * 使得 fusefs-client 完全独立于 CSI Pod，单独作为 Fuse Pod 运行，CSI Pod 作为控制平面来 create/delete Fuse Pod，并结合 CSI NodeStageVolume/NodeUnstageVolume 和 NodePublishVolume/NodeUnpublishVolume 机制实现代码简洁化，相比于 juicefs-csi 实现逻辑。(已经向 juicefs-csi 社区反馈)

后续：独立调研 eBPF 在 linux 磁盘 I/O 上的性能优化，包括把 fuse 用户态的一些接口对应的业务逻辑下沉到内核层去执行，无需再从内核态到用户态的一次数据复制，从而达到更高性能。
调研发现社区内已经有相关的尝试和文档，不过还不成熟，且貌似还需要修改 linux 文件系统相关代码，重新编译 linux 内核(存疑)。总之，由于时间精力有限，无法继续深入调研。

熟悉 raft 共识算法，熟悉 linux vfs fuse 机制和 linux 磁盘读写 I/O 机制，熟悉分布式文件存储内部机制

etcd-operator 开发：
对 etcd 机制和内部原理比较了解

### (7) log-operator 以及 filebeat processor 二次开发
基于公司内搜索部门的特殊需求，二次开发一个 filebeat processor 插件，并编译进 filebeat 源码内作为公司内部维护版本。
同时为支持

并对 filebeat 源码和运行机制有一定了解。


# 个人描述
做事情认真负责，喜欢团队合作。
对云原生感兴趣，主要关注 kubernetes 生态相关技术，包括但不限于网络、分布式存储、监控、日志和网关相关技术。对 eBPF 应用于容器网络和可观测性比较感兴趣。





# 面试

## 数据结构和算法
(1)fusefs-client 中的 LRU(哈希表和双向链表)
(2)fusefs meta-node 中的 b-tree，以及 b+tree
(3)优先级队列(平衡树、最小堆、最大堆)

(4)单向链表和双向链表
单链表反转，

(5)二叉树

(6)LRU数据结构
