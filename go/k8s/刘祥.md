# 个人介绍
* 姓名：刘祥
* 性别：男
* 出生日期：1991-10-10
* 学位：北京理工大学学士(2008-09 ~ 2012-07)/北京航空航天大学硕士(2012-09 ~ 2015-07)
* 工作经验：7 年
* 毕业时间：2015-07-01
* 联系电话/微信号：13426116367
* 电子邮箱：lx1036@126.com或lx20081036@gmail.com
* 目前职位：奇虎360技术中台基础架构组容器云技术专家
* 技术专栏：https://juejin.cn/user/800100194726088/posts
* 应聘职位：云原生研发工程师(Kubernetes方向)
* K8S证书：CKA 证书

# 工作经历及项目经验

## 北京当当网信息技术有限公司(2015-07 ~ 2016-07)
主要使用PHP语言重构一些老业务代码和迭代业务新功能，主要工作内容包括：
(1)负责当当图书和店铺域的改版和优化，并负责后续版本迭代工作。
(2)负责当当优品馆全面改版项目，对一些老代码进行了重构优化，提高代码可读性。

## RightCapital(2016-07 ~ 2019-07)
参与创业，加入时公司共5个人，北京和纽约办公室各2-3个人。作为全栈工程师参与创业，写后端和前端业务。主要是用 PHP 语言和 Laravel Web 框架做一款金融软件，面向美国市场。主要工作内容包括：
(1)使用 PHP 框架 Laravel 编写金融软件 RightCapital 后端的 Restful API，并使用 PHPUnit/Mockery 编写单元测试和集成测试。同时，结合业务需求，对 Laravel 做了很多二次开发，并做成共享私有包，并编写 API 的 Swagger 文档。

(2)使用 Angular 作为前端，Laravel 作为后端，并使用 Ant Design 组件库编写 Admin 后台，供美国客服团队使用。重写金融软件 RightCapital 前端模块，把其从 Angular.js 重写升级到 Angular 框架。

(3)运维云服务器AWS，搭建一些 DevOps 软件工具，如Gitlab CI/CD、编写 Docker images 等等，并使用 Terraform/Ansible 开发一些提高工作效率的工具等等。

## 奇虎360(2019-08 ~ 至今)
在360技术中台容器云基础架构组负责全公司的 K8S 集群开发和维护，主要工作内容包括：

### (1) 在离线混合部署项目开发
开发人员：2个人开发整个项目，人员投资太少、上层支持不够且项目非常复杂难搞，导致整个项目周期非常长，且至今一直在进行中。

背景：目前公司内部生产 K8S 部署的业务 Pod 绝大多数都是在线业务 Pod，白天吃资源多晚上吃资源少。而大数据和人工智能离线业务逐渐开始 K8S 化后，这些离线业务 Pod 白天吃资源小晚上吃资源大，为了进一步压榨机器资源，就有了两种类型业务 Pod 混合部署。但是，如果资源没有隔离，离线业务 Pod 会严重干扰在线业务 Pod，导致在线业务 Pod QoS 服务质量不可接受。所以，混合部署必须做好资源隔离。

#### (1.1) 资源隔离 agent 开发
(1.1.1) CPU 资源隔离
CPU 资源隔离主要解决两个问题: 对离线业务 Pod 做好资源压制，使得离线业务 Pod 整体 CPU 消费不超过规定的阈值；CPU 超线程干扰问题，根据 CPU NUMA 架构，如果一个物理核两个逻辑核下，一个逻辑核跑在线 Pod，对端逻辑核跑离线 Pod，离线 Pod 会严重干扰在线 Pod。

早期阶段基于社区 linux 4.19 kernel: 使用物理核直接物理隔离来避免超线程干扰，开发的 agent 先读取 node status 上在线业务 Pod 实际 cpu 使用量(值由我们写的一个 webhook 读取 metrics-server 后经过比率计算 patch 上去的)，然后按照公式 (node.Allocatable - 在线pod.Used) * PolicyRatio 得到留给离线 Pod 的资源使用量，并且保证必须是偶数，比如 4 个 cpu 逻辑核。然后 agent 会 list 所有离线 Pod 的业务容器，并更新其 cpuset 值为定义的 4 个 cpu 逻辑核，并且这 4 个逻辑核是按照 NUMA 架构分配的，比如机器24个核，4 个逻辑核编号就是cpuset[0,11,1,12]，即 2 个物理核，更新每个容器的 cpuset 是通过调用 ContainerRuntime UpdateContainerResource api 去更新的。然后更新所有在线 Pod 容器 cpuset 为剩余的 20 个逻辑核，即 10个物理核。最后，更新在离线 Pod 的容器是周期性去计算更新的，这样导致如果在线流量突然暴增 CPU 消耗过大，不能快速响应，影响在线 Pod QoS。

总之，通过使用 cpuset 物理核完全隔离来避免超线程干扰，但是这种通过周期性更新方式响应比较慢，不能很好解决在线 Pod 突然暴增需要减少分配给离线 Pod 的资源，而且这种方式也没有更好压榨机器资源。并且为防止更新 cpuset 冲突，kubelet cpu manager 只能是 none policy，不能是 static。

后期阶段基于 Aliyun linux 4.19 kernel: 由于公司和阿里技术方面的合作，后期在和阿里的交流中知道，阿里开源内核在内核层面非常好的支持了混合部署场景下，
对各种资源做了很好的隔离，这些隔离特性通过 cgroup 暴露，包括：

GroupIdentity 功能，基于进程优先级的调度，而不是 linux 默认的公平调度 CFS，通过设置在线 Pod 容器 GroupIdentity cpu cgroup 更高优先级后，当在线业务 Pod 突然暴增需要更多资源时，GroupIdentity 可以快速响应暂时压制离线业务 Pod，而且这个设置只需要设置一次就行。我们自研的 agent 会去更新 K8S burstable/guaranteed Pod 的 GroupIdentity cpu cgroup /sys/fs/cgroup/cpu/kubepods/burstable/cpu.bvt_warp_ns 
和每一个 guaranteed Pod /sys/fs/cgroup/cpu/kubepods/podID/containerID/cpu.bvt_warp_ns 为 2，优先级最高，而 /sys/fs/cgroup/cpu/kubepods/besteffort/cpu.bvt_warp_ns 为 -1，优先级最低。同时还需要自研一个 webhook 拦截创建的 Pod，把离线 Pod spec.resources 全部置空并存入 annotation 里，这样这个离线 Pod 就会被置于 /sys/fs/cgroup/cpu/kubepods/besteffort 目录下，然后 agent 会根据 Pod annotation 里的值还原容器的 cpu.shares 等值；

因为有了 GroupIdentity，就可以使得在离线业务 Pod 可以使用在一个物理核上，这样就可以使用 cpu.cfs_quota_us 来实现 CPU Suppress 压制，比如根据设置的压制离线 Pod 的水位线 cpu_suppress_threshold， 周期性的计算得出需要压制离线 Pod 的资源 cpu_suppress = node.Allocatable * cpu_suppress_threshold - 在线pod.Used，然后周期性的更新离线业务 Pod 的 cpu quota 值 /sys/fs/cgroup/cpu/kubepods/besteffort/cpu.cfs_quota_us；

总之，基于阿里内核开发混部上层套件后，在离线业务 Pod 无需通过 cpuset 来物理核隔离，而是都可以运行在所有逻辑核上，比如 cpu0-24。并且，阿里内核可以做到快速响应在线 Pod 资源暴增需求，快速压制离线业务 Pod，而且还不干扰在线业务 Pod QoS 服务质量，完美解决了超线程干扰问题。

最后，由于和阿里有战略合作，有幸和阿里内部人员共同验证过阿里混合部署的 K8S 套件，做过 POC 验证测试，包括 CPU/Memory/Network 隔离等等，最重要的指标，对在线业务 Pod 的干扰率小于 5% 以内，的确强悍。

(1.1.2) Memory 资源隔离
早期阶段基于社区 linux 4.19 kernel 没有做基于内存的隔离。

后期阶段基于 Aliyun linux 4.19 kernel: 容器内存接近 resources.limit 内存上限或者整机内存不足时，会触发 kernel 内存直接回收，这会影响业务容器的内存申请，所以需要 kernel 支持内存异步回收。阿里内核通过设置 cgroup memory.wmark_ratio(0-100) cgroup 值在内核层支持异步回收，当内存使用量达到 wmark_ratio 比率时，内核就开始回收内存，这样业务 Pod 申请内存时不再需要等待内核去释放内存而是直接有足够的内存去申请，提高业务的性能。该功能只需要我们开发的 agent 去设置 memory.wmark_ratio 值就行，比较简单。

(1.1.3) Disk I/O 资源隔离
直接使用两块盘做隔离，离线业务 Pod 使用一个盘，在线业务 Pod 使用另一个盘。

#### (1.2) Pod 调度器 scheduler plugin 自定义开发
内容一：人工智能训练作业需要调度器批量调度 Pod，而目前 K8S scheduler 的 plugins 是单个 Pod 调度，需要开发一个 batch-pods plugin 来支持一组 Pod 批量调度，并且支持如果 Pod 数量满足最小数量就认为该组 Pod 可以调度。同时该 plugin 还得支持 namespace 级别自定义 GPU 配额限制去调度(配额限制通过 GPU-Quota CRD 定义，在 plugin Filter 阶段去根据该 namespace-scoped CR 去做 Filter)。

难点：在 PreFilter/Filter 阶段根据 GPU-Quota CRD 来判断是否还有 GPU resource 余量供该 namespace-scope Pod 调度；结合 K8S Scheduler Framework 的 Permit plugin 机制，在 Permit 阶段判断当前组内的 pods 数量总和是否满足 BatchPod CRD 定义的 minNumber，如果不满足则放入 WaitingPods 内等待其他兄弟 pods 调度，直到数量总和满足则在该 Pod Permit 阶段内放行所有其他兄弟 pods，这样就实现了可以按组级别调度 pods，这里难点是利用了 K8S Scheduler 的 scheduling cycle 内的 Permit plugin，和 bind cycle 是 goroutine 异步的，且在该 goroutine 内会首先阻塞等待 Permit 的结果是否放行当前 Pod 进入下一步 Bind plugin，而我们的 batch-pods plugin 是在 Permit 里做文章。

内容二：混合部署时，需要根据 Node 上给离线 Spark Pod 作业分配的自定义资源 colocation/cpu 和 colocation/memory 来调度 colocation-spark plugin。尽管 K8S Scheduler NodeResourcesFit plugin 默认支持过滤自定义资源，但是我们这里还需要根据策略来过滤，所以需要自定义开发一个 plugin。

难点：这里主要是 Pod resource 内有自定义资源，调度器在 PreFilter/Filter plugin 阶段读取 node allocatable 中 colocation/cpu 和 colocation/memory，并按照不同的策略来选择不同的 ration 比率计算出可被离线 Pod 调度的还剩余资源，并进行过滤。而 node allocatable 中 colocation/cpu 和 colocation/memory 是由一个 controller 去 watch NodePolicy CRD 通过计算再去更新 node status 中获得的。plugin 难点不大，比较简单。同时，还需要开发一个简单的 controller 去更新 node status 中自定义资源 colocation/cpu 和 colocation/memory 的值。

现状：目前少批量 K8S 采用的是我们自研的 Scheduler，所有在线和离线 Pod 都使用这个 Scheduler，缺点是对离线 Pod 调度支持还严重不足。

后期调研：针对人工智能和大数据部门的一些需求和整体规划，针对混合部署的场景需求下，正在调研专门调度离线 Pod 的调度器华为开源的 volcano。离线 Pod 使用 volcano 调度，在线 Pod 使用 K8S default Scheduler，但是这会引发多调度器资源冲突问题，目前正在调研相关的解决方案，最好能够做到只使用一个 Scheduler，既可以调度在线 Pod，又可以调度离线 Pod。这里我们在和阿里合作时候，知道阿里的方案是统一调度器，K8S 里只有一个调度器，既可以调度在线 Pod 又可以调度离线 Pod，这也是接下来开发的目标。

#### (1.3) 网络带宽隔离和限制相关的调研
常用的方式是针对每个容器的带宽限制，比如社区常用的 cni plugin 提供了基于 Pod 的 ingress/egress 带宽限制。但这种方式限制很多，网络资源利用率也一般，只能作为一种补充方案。一般混合部署场景下，根据流量优先级划分来实现网络带宽隔离，同时还得需要保证网络资源竞争中，在线业务的流量可以对离线业务的流量进行压制。

网络带宽隔离我们没有开发相关套件，但是做了 egress 侧相关方案调研：
创建好 tc qdisc 和多个 tc class classid 之后，不同 classid 带宽不同，比如千兆网卡 1Gbit，可以创建两个 classid: low 和 high，low 占用 200Mbit, high 占用 800Mbit，其 classid 分别为 1:3 和 1:5。然后把 low classid 1:3，即 0x10003，写入 net cgroup 中如 /sys/fs/cgroup/net_cls/test/net_cls.classid，就可以实现 test 容器占用该机器网络带宽最多 200Mbit。设想是把所有离线 Pod 的带宽都设为 low classid，在线 Pod 的带宽都设置为 high classid 就行，这个可以通过我们自研的 agent 来周期性设置，不过目前还没有开发这个功能。同时还需要考虑到，在线业务网络带宽流量开始暴增，需要压制离线业务网络带宽流量，这时整个离线业务网络带宽流量就要从 200Mbit 开始压制，实现有些难。

后续在和阿里合作时 POC 测试了阿里的方案：egress 侧阿里方案类似，但是却可以实现流量压制，由于对方是商业产品不开源，无法得知具体做法和代码，只是做了 POC 验证测试。

ingress 侧不好做，我们起初探索的方案是：把机器的主网卡的流量 redirect 到一个 ifb 网卡，然后在 ifb 网卡出口方向做带宽限制，方法基本和 egress 侧一样，区别是下发 tc filter 匹配规则时需要去 match dst podIP。根据这个方案我们也做了验证性测试，但是由于我们的 CNI 是使用的 Cilium，该方案在 Cilium 下不太好做。

后续在和阿里合作时 POC 测试了阿里的方案：需要交换机侧给回包流量打上标签，这需要网络部门配合配置下交换机，然后使用一个 agent 去配置网络相关 cgroup，同时也具有网络压制功能。由于商业产品，无法得知具体做法和代码，只是做了 POC 验证测试。

现状：网络隔离目前还处于调研方案阶段，以及和阿里方案的技术交流阶段。

#### (1.4) 总结
相关经验：对 kubelet CPU 管理和 CPU NUMA 机制都比较了解。K8S Pod 调度器的内部机制有不错的了解和实践，包括整个 plugin 架构和调度队列数据存储等机制。对在离线混合部署和大数据上 K8S 有一定的实践经验，尤其是大数据上 K8S 使用到的 spark-operator 等组件源码，都比较了解。

现状：由于数据库中间件和大数据容器化进度太慢，导致容器开发的混部方案，很多都只在测试 K8S 里测试使用，没进入生产 K8S。

后记：今年阿里开源了其混部框架 koordinator，尽管我们去年自研以及和阿里合作时，阿里还没开源这一套。同时，从其内部老大那得知，这个混部框架只是一个可用的通用性框架，并不是其阿里混部的全部套件，各个公司可以根据该框架自己做定制开发，其阿里混部套件是一个商业产品。下半年开始，我将带领人员以该框架为主，把我们的功能逐渐移步到该框架上，以该框架再做二次开发，同时也积极参与这个混部社区。

### (2) 负载均衡项目 LoadBalancer Operator(依赖外部 LVS)，实现暴露 K8S 集群内 Pod 为集群外提供服务
开发人员：独立开发整个项目。

内容：替换旧的通过 externalIP service 对外暴露服务方式(内部俗称边缘节点方式，与 NodePort 方式类似)，使用 K8S Pod 直连公司已有的生产 LVS (keepalived 搭建)集群方案，实现集群外部流量由 LVS 集群接管。K8S 集群内部四层负载均衡由 kube-proxy 组件来接管(部分 K8S 由 Cilium eBPF 替换 kube-proxy)，外部由 Cilium+BGP(bird) 来宣告 Pod Cidr。最终，实现 LVS VIP 直连 PodIP，并结合 BGP 宣告分配给每一个 Node 的 Pod Cidr，使得 PodIP 公司内网可达，实现包从 LVS 集群直接跳转 Pod 所在的 Node，减少网络跳转，提高网络性能，实现了服务对外暴露方式。
初始版本把 LVS 相关配置放入 ClusterIP Service Annotation 里，后续使用 LoadBalancer CRD 模式重构了一版本并生产可用，使得配置更简单，且更具有可观测性。

难点：因为调用 LVS API 更改 LVS 配置后，需要等待 28s 后 LVS 才会下发配置规则，所以在删除 Pod 时，需要在 LVS 侧 RS 被删除之后才能删除 Pod，这里需要利用 webhook 给 Pod 打上 PreStop lifecycle graceful-shutdown 保证 30s 之后 Pod 才会被删除；

滚动更新在新建 Pod 时，极端情况下 LVS 侧 RS 创建失败，如果新 Pod 创建成功且立刻 ready 后，旧 Pod 会滚动更新被 kubelet 删除，导致 LVS 下没有新的 RS 而旧 RS 全部删除，流量严重损失，这是完完全全不可接受的，所以只有在 LVS 侧 RS 创建成功时才会使得 Pod 为 ready 状态，再依次滚动下一个 Pod，所以利用 webhook 给 Pod 打上一个自定义的 ReadinessGate，RS 创建成功后再使能 Pod 为 ready 状态；

LVS 侧不稳定，调用删除 RS API 28s 后 RS 规则还存在，而此时 Pod 已经删除，导致流量丢失，webhook 会拦截删除 Pod 请求并先去 LVS 侧做检查，缓解此类极端情况(这只有部分机房 LVS 有这样的问题，所以也只有部分少量的 K8S 会有这个 webhook，因为该 webhook 会拖慢删除 Pod 效率)。同时部署的 Deployment 周期性去同步 LVS 侧 RS 和 K8S 侧的 Pod 列表来进一步缓解此类极端情况；

总之，由于 LVS 很多莫名其妙和不可抗因素，Operator 需要充分考虑这些极端情况。

现状：大多数生产 K8S 集群已经使用该方案作为暴露 Pod 服务的优先方案。

### (3) 负载均衡项目 LoadBalancer Operator(不依赖外部 LVS)，实现暴露 K8S 集群内 Pod 为集群外提供服务
开发人员：独立开发整个项目。

背景：LVS 集群不稳定尤其性能原因(RS数量过大后 keepalived 配置更新容易失败导致应该删除的 RS 没有删除等问题，造成流量丢失，目前临时解决方案是 LVS 定期重启)，以及 to B 交付时无法交付 LVS 套件(目前只有 NodePort 方案)，所以必须探索出一套不依赖于外部 LVS 的暴露服务的方案。公司内使用作为 LoadBalancer Operator(依赖外部 LVS) 方案的补充，公司对外交付时的另一种方案选项。

内容：替换 LoadBalancer Operator(依赖外部 LVS)方案，借助 K8S 集群内已经实现了四层负载均衡的机制，如 K8S LoadBalancer Service 机制，开发 K8S LoadBalancer Service IPAM Operator (Deployment 部署)来给 LoadBalancer Service 分配 ServiceIP，且支持多个 ServiceIPPool 以及指定特定的 ServiceIPPool。

同时为了使得该 ServiceIP 也在集群外可达，开发 LoadBalancer Service BGP Speaker 宣告 ServiceIP(Daemonset 部署)，且通过 BGPPeer CRD 配置，使得 LoadBalancer ServiceIP 在集群外可达，可以不再依赖外部 LVS 集群，且网络跳转数量更少，性能更高。并且支持 Service ExternalTrafficPolicy 功能。

同时为了替换 K8S 集群内部署的 bird Daemonset 软件来宣告 Cilium Operator 给每一个 Node 分配的 Pod Cidr，因此开发了 Pod Cidr BGP Speaker 来宣告，使得 PodIP 在集群外可达，目前已经在部分生产 K8S 集群上线。

该方案相比于 LoadBalancer Operator(依赖外部 LVS) 优点：不再依赖于外部 LVS 集群，交付更简单；网络跳转更少，性能更高；Pod 滚动时为了减少流量丢失，依赖外部 LVS 方案使用了 ReadinessGate 等一系列缓解方案，而本方案则把 Pod 滚动交给了 kube-proxy 或 Cilium 去解决，不存在流量丢失问题。该方案相比于 LoadBalancer Operator(依赖外部 LVS) 缺点：依赖 BGP 宣告路由，且每一个 ServiceIP 对应多个 Node 的路由，导致上层交换机路由数量会变大。

现状：公司内部作为 LoadBalancer Operator(依赖外部 LVS) 方案的补充使用，公司外部交付优先考虑的方案。

### (4) Cilium IPAM Operator 项目
开发人员：独立开发整个项目。

内容：针对 K8S 集群内需要根据 nodeSelector 选择不同的 IPPool 需求，比如集群内有些 worker nodes 是专属某个业务的，且该业务因为调用外部服务需要使用另一个 Pod cidr。并且尽可能需要一个 Node 可以配置多个 PodCidr，实现当 Node 的 IP 资源不足时，可以动态扩容 IP，该需求类似于 Calico 支持多个 IPPool 功能，且公司业务需要该功能。

难点：Cilium 目前支持多个网段 IPPool，只有前一个网段 IPPool 耗尽才会使用下一个网段 IPPool，目前我们有的生产 K8S 使用了这个功能。但是有些生产 K8S，需要支持根据 nodeSelector 选择不同的 IPPool 需求，且最好支持一个 Node 可以配置多个 PodCidr，但 Cilium 不内置支持。所以需要根据 Cilium IPAM 自定义开发，目前已经根据两种不同的 Cilium IPAM 分别开发了对应的 Operator。

#### (4.1) Cilium Kubernetes IPAM Operator
根据 Cilium Kubernetes IPAM 机制开发自定义的 Operator，结合 cilium-agent 从 K8S Node 或 Node Annotation 中读取 PodCidr 机制，所以选择关闭 kube-controller-manager 给 K8S Node 分配 PodCidr，开发自定义 Operator 来根据 Node Labels 选择不同的 IPPool 并分配对应的 PodCidr，并添加到 Node Annotation 中，从而实现了不同 K8S Node 可以选择不同的 IPPool 需求。

总之，目前已经生产可用，经过一段时间使用，符合业务需要，且不需要更改 cilium-agent 以及 BGP 相关的配置参数。
其实该方案是我从 Cilium 源码里扒出来的，有点点 hack，起初担心 Cilium 后续版本会不会修改从 Node Annotation 中读取 PodCidr 行为，在 Cilium 官方 Slack channel 里问了官方人员，说应该不会，这个机制也是为了兼容以前版本的机制。

#### (4.2) Cilium CRD IPAM Operator
更进一步，第一种方案尽管已经满足业务需要，但是该方案不支持一个 Node 配置多个 PodCidr，而资源更大的 Node 可以支持更多 PodIP，导致反而 PodIP 不够用。所以更进一步，选择 Cilium CRD IPAM 机制，再次开发自定义的 Operator，支持按需动态扩容和回收节点的 PodCidr IP 资源。

在 Cilium CRD IPAM 模式下，CiliumNode 存储的是一个个可以被分配出去的 IP，可以在这里设计这些 IP 来自于多个 PodCidr，从而实现了一个 Node 支持多个 PodCidr。同时为了保证这多个 PodCidr 被宣告出去，还得更改 BGP speaker，以及 cilium-agent 的一些默认行为(通过更改 Cilium 配置参数来实现)。

缺点：该方案实现复杂，同时需要更改 cilium-agent 相关参数，比如 cilium-agent 默认只会为有且仅有一个 PodCidr 创建一个路由指向网卡 cilium_host，为了支持多个 PodCidr，需要开启每一个 Pod 一个路由的配置参数，而出于性能考虑这不是 Cilium 的默认行为，等等几个其他参数配置；同时还需要修改 BGP speaker 软件 bird 的配置。

总之，该方案有点 hack。总之，出于稳定性考虑以及业务需要考虑，暂时在测试 K8S 集群部署使用。

现状：出于稳定考虑，目前生产 K8S 先小批量使用 Cilium Kubernetes IPAM Operator，后续再考虑使用 Cilium CRD IPAM Operator。

后记：这周刚刚发布的 K8S v1.25 版本 kube-controller-manager 开始支持通过配置 ClusterCIDR CRD 实现多个 IPPool 功能。同时由于 Cilium 有 Kubernetes IPAM mode，会把 podIP IPAM 交给 kube-controller-manager 来处理，目前正在调研是否可以使用 kube-controller-manager v1.25 结合 Cilium 就可以做到根据 nodeSelector 来实现多个 IPPool 功能。如果能做到，等以后生产 K8S 升级到 v1.25 就可以使用 K8S 内置的功能，废弃我们自定义开发的组件，跟着官方走。

### (5) 自定义容器网络插件 VPC-CNI 项目
开发人员：独立开发整个项目。

#### (5.1) VPC-CNI
背景：由于网络部门决定后续公司内交换机不再支持 BGP(由于 BGP 路由太多导致很多维护问题)，以及容器网络需要和虚拟化团队提供的 VPC 网络相融合，部门从上而下开始推动容器网络逐渐向 VPC 网络靠拢，需要容器网络和 VPC 网络在同一个网络平面，需要容器团队提供相应的解决方案。

内容：解决方案是利用虚拟机弹性网卡绑定多 IP 机制，来打通容器网络平面和虚拟机网络平面，即调用 VM API 给该 VM 创建并挂载一个新的弹性网卡，并绑定多个辅助 IP 给 Pod 使用，这些辅助 IP 是和虚机 IP 在同一个 VPC 网段内，这样就实现了集群内同 Node 和跨 Node 的 Pod 相互通信，以及集群外访问 PodIP 通信问题，访问 PodIP 和访问 VM IP 没有任何区别。整个过程无需 BGP 去宣告路由，PodIP 集群外可达由 VPC 网络去解决，且不存在不同规格 Node 的 PodIP 资源浪费问题。且和 VPC 网络完全融合，容器团队无需再和网络部门沟通 BGP 网络问题，VPC 网络问题都交给虚拟化团队去负责。

针对容器上虚机场景，需要开发 CNI Bin 组件和 IPAM Daemon 组件。
(1) Bin 组件完全实现 CNI 标准接口函数，包括创建和销毁 Pod 的接口函数。Bin 组件 grpc 调用 IPAM Daemon 组件获取 PodIP，并创建 ipvlan 网卡 eth0 置于 Pod 内，该 eth0 的 parent 网卡是弹性网卡(弹性网卡通过调用 VM API 创建挂载)，采用 ipvlan L2 mode，即弹性网卡作为二层网桥。并配置 PodIP 到 eth0 网卡上，配置默认路由(出口网卡是容器的 eth0，网关地址是其弹性网卡的网关地址)，以及 arp 表。因为 linux ipvlan 网卡特性，容器的 eth0 网卡的 mac 地址使用的是弹性网卡的 mac 地址。南北向，容器内的流量走弹性网卡直接出去，mac 地址直接使用弹性网卡 mac 地址；外部访问容器 IP，流量直接经过该弹性网卡进入容器。东西向，跨节点 Pod 互访也是相同，流量都是通过对应的弹性网卡。这样，容器的网络平面和虚机的网络平面在同一个网络平面，访问容器 IP 和访问虚机 IP 走的网络链路相同。

最后，东西向访问 serviceIP 四层负载均衡，由于容器内的网络 namespace 没有 kube-proxy 下发的 ipvs 规则，所以容器内访问 serviceIP 需要把流量 redirect 到 host net namespace 中。这里使用了 tc filter 来做(这里参考了限速 Pod ingress 方向流量时使用 tc filter 把流量 redirect 到 ifb 网卡，在 ifb 网卡 egress 方向做带宽限速)：先创建一个 clsact qdisc 排队规则，然后每一个弹性网卡如 eth1 会在 host net namespace 中创建一个 ipvlan eth1_slave，然后在弹性网卡 eth1 的 egress 上下发 tc filter 规则，在 selector 选择器里去筛选 dst cidr 是 service cidr，action 里去 mirror redirect 流量到 eth1_slave ipvlan 网卡上，因为 eth1_slave 在 host net namespace 中，就会走 ipvs 规则 DNAT 到 podIP，然后从弹性网卡 eth1 出去，回包会根据配置的路由进入 eth1_slave ipvlan 网卡。这样就解决了容器内访问 service cidr 问题。

(2) IPAM Daemon 组件主要调用 VM API 来给 Pod 申请 IP，包括弹性网卡创建删除 API、弹性网卡挂载卸载 API 和为该弹性网卡申请释放 IP 的 API 等等。IPAM Daemon 主要实现 rpc 定义的 AllocateIP 和 ReleaseIP 两个接口，作为 grpc server 的 service 被 Bin 组件调用，比如当新建 Pod 时会调用 AllocateIP 接口获取 PodIP。由于我们的 VM API，比如创建挂载弹性网卡、申请 IP 等 API 一般都是很久时间才会生效，是很慢的，如果创建销毁 Pod 时同步去调用 VM API，会导致配置 Pod 网络资源很慢，所以需要异步提前申请 IP 资源，实现资源池化。这里我设计资源池 IPPool 对象，该 IPPool 会周期检查 idle IP 是否在 [minIdle, maxIdle] 配置的范围内，如果小于 minIdle，则依次检查每一个弹性网卡所绑定 IP 数量是否到最大值，没有则继续调用 VM API 申请 IP，直到 idle IP 数量达到 [minIdle, maxIdle] 配置的范围内。如果所有的弹性网卡所能绑定的 IP 数量已达最大值，但是还没达到配置范围内，则会异步创建挂载并带有一定数量辅助 IP 的新的弹性网卡。如果 idle IP 大于 maxIdle，则调用 VM API 给该弹性网卡释放回 IP 资源。并且，占用和释放 Pod IP 回 IPPool 中，需要主动检查 idle IP 是否在 [minIdle, maxIdle] 配置的范围内。并且，idle IP 是个优先级队列，新建的 IP 置前，当新建 Pod 时优先使用最旧的 IP，当销毁 Pod 时则把 PodIP 置于 idle queue 的队尾。并且，把所有 Pod 已经在使用 IP 资源存入 IPPool inuse 中，同时也存入 boltdb 存储并落盘，这样重启后可以从 boltdb 文件 restore 回 inuse 和 idle 中，作为 IPPool idle/inuse 的初始状态。最后，这样就实现了弹性网卡辅助 IP 的资源池化，当空闲 IP 小于资源池最小水位时则自动补足 IP 资源，当空闲 IP 大于资源池最大水位时则自动释放 IP 资源，保障了资源的高效利用和分配效率。

难点：
(1)容器内访问 service cidr 如何走 kube-proxy 下发的 ipvs 规则。
(2)IP 资源池化架构设计，代码实现有难度，且复杂，需要考虑并处理一些性能边界问题，比如几乎同时为多个 Pod 申请 IP 资源时，需要聚合从 buffer channel 中的请求，聚合后再一次请求后端 VM API。

不足：还没有实现基于 Pod 的 ingress/egress 网络带宽限制 bandwidth，这块也不难，主要就是下发 tc class/filter 规则，目前由于需求和精力原因，还没有实现；还没有完成固定 podIP 功能，根据业务需求后续版本中实现，有业务需要但是精力原因，暂未完成。

现状：目前已经在几个规模较小的生产 K8S 里开始使用。

#### (5.2) CNI NetworkPolicy
背景：由于需求方希望能在虚机上部署的 K8S，最好也能支持网络安全 NetworkPolicy 的功能，包括不同 namespace 间的 Pod 网络隔离，以及只希望特定网段 cidr 访问其服务，所以需要容器团队提供对应的方案。

内容：因为虚机上部署的 K8S 不再使用 Cilium CNI，而是基于我们自定义开发的 VPC-CNI。所以起初打算开发一个 controller 根据 NetworkPolicy 对象去往 filter table 下发 iptables 规则，后期调研 Cilium 可以单独部署作为 NetworkPolicy Controller 组件，实现 NetworkPolicy 网络安全的所有功能，目前正在调研部署 Cilium 来解决这个需求。

### (6) fusefs-csi 和 fusefs 项目
背景：由于 cephfs 性能不好，还时不时出现 ceph-fuse 进程 hang 住了导致这个节点卡死，影响了该节点上的所有 pods 使用。以及 cephfs 运维越来越难，经常小事故不断，所以决定自研一套基于容器环境的分布式文件存储系统 fusefs。

#### (6.1) 开发 fusefs 分布式文件系统项目：fusefs 项目包含 fuse-client、master(raft) 和 meta(multi-raft) 三个模块
开发人员：3个人开发整个项目。

(1)主要参与开发 fuse-client 模块，fuse-client 进程置于 fusefs-csi Pod 里，为每个 K8S 节点上的每个 pv(可以被多个 Pod 挂载) 启动一个 fuse-client 进程来实现 fuse 挂载，该进程实现了 linux fuse vfs 的接口，真实数据存储在远程 S3 上。主要参与开发 file/directory 的 create/delete 相关函数，比如创建一个 file 时，从 master 模块分配给该 volume 的 3 个 meta partition 中选择一个 partition，并 TCP 调用该 partition 中的 leader 节点来创建 inode/dentry。同时为了高性能，inode 和 dentry 数据结构都缓存在 fuse-client 进程的本地内存中，其中 inode 使用 LRU 数据结构存储，根据其有效时间来从 meta partition 刷新 inode，这样 read file 时会先从 inodeCache 中获取 inode，然后调用 S3 gateway api 根据该 inode 从 bucket 中获取该 file 的真实数据。另外，我还修改了第三方 fuse 包，使其支持 macOS 系统，使得可以 mac 本地运行测试，这个是第三方 fuse 包的一个 bug(存疑)，我还没有回馈社区。

备注：S3 是我们组基于开源的 nebulas 二次开发并完全支持 S3 协议，并优化了一些 api 提高了性能，比如 update object 时会根据 chunk 更新，即一个 1G 文件更新一个字节，不会去先 read 1G 文件修改一个字节再去 write，而是 1G 文件有 1024 个 chunk，每个 chunk 1M，只会 read 1M chunk 再去 write 回去。

待优化点：在 fuse-client 进程缓存 volume 的文件数据。

(2)主要参与开发 master 模块，master 作为控制平面一般部署3节点，使用 raft 保证数据强一致性。主要参与开发 meta node 注册相关 HTTP api 开发，meta node 个数可以无限扩展，每个 meta 进程启动时都会调用 master 的 add-meta-api 注册，并把 add-meta cmd 相关 key-value 数据提交到 raft，状态机是会把这个 key-value 数据存储到本地的 boltdb 文件中；主要参与 create/delete/expand volume 等相关 api 开发，这些 api 会被 fusefs-csi 调用，在 volume 创建时会先把 volume 按照一定 stepIndexID 切割成 3 个 partition(每个 partition 有 start~end index 范围，比如 p1,p2,p3，每一个 partition 通过 TCP 调用 meta node api 创建)，对于每一个 partition(比如p1)根据 meta node 使用率等一些策略选择默认数量为3的 meta node，并形成一个 raft 实现数据多副本，add-partition、add-volume cmd 等相关 key-value 数据提交到 raft，状态机是会把这个 key-value 数据存储到本地的 boltdb 文件中。所有的 raft log 和 conf 数据都会存储在 boltdb 中，raft 使用的开源的 hashicorp/raft 库。

(3)部分参与 meta 模块，meta 节点可以无限扩展，该模块主要在内存里以 btree 存储 inode/dentry 元数据。一个 volume 默认分割成三个 partition(p1,p2,p3)，每一个 partition(比如p1) 由 raft 保证强一致性，即使用 multi-raft 保证数据强一致性，对于每一个 partition 其 raft fsm 状态机为两个 btree inode 和 dentry。当 fuse-client 通过 TCP 调用创建 inode/dentry 时，从其请求参数中的 partitionID 获取 partition，并提交 create-inode/create-dentry 等相关 key-value 数据提交到 raft，实际就是读写状态机两颗 inode/dentry btree，使用 btree 也是因为该数据结构读写效率高。

待优化点：multi-raft 使用的开源库，代码比较老旧且不优雅，设计一些地方也不合理，比如 heartbeatPort 和 replicaPort 不是同一个端口等等，后续考虑要么自研要么进一步优化这个开源库；为了高性能，所有的 inode/dentry 都保存在内存里的两个 btree，导致随着文件数量越来越多(尤其人工智能业务的用户一个 volume 可能有几千万个文件)，内存也越来越大，可能有200-300G，后续考虑把一些不太热的数据落盘；fusefs 和 fusefs-csi 目前还没有支持 snapshot api。

#### (6.2) 开发 fusefs-csi 项目：为团队自研的分布式文件存储实现一个 K8S CSI，方便业务 Pod 动态挂载 fusefs pvc
开发人员：独立开发整个项目。

内容：CSI 实现了 IdentityServer service，借助 node-driver-registrar sidecar 容器向 kubelet 里注册当前 CSI driver；

CSI 实现了 ControllerServer service 接口，当用户创建 pvc 时，PVController 通过给 pvc 打 annotation 和 external-provisioner sidecar 容器交互，然后该 sidecar 容器作为 CSI client 调用 CreateVolume/DeleteVolume/ExpandVolume 等 ControllerServer service 接口，在这些接口中会调用 fusefs master 组件提供的 HTTP API；

同时，CSI 也实现了 NodeServer service 接口的 NodeStageVolume/NodePublishVolume 等接口，这里利用了 NodeStageVolume 接口解决了当同一个节点调度多个 Pod 相同副本时，只会在该节点内启动一个 fuse-client 进程，从而提高了资源利用率，在第一个 Pod 副本调度到这个节点时，会启动有且仅有一个 fuse-client 进程，后续 Pod 副本不会启动，fuse-client 进程挂载点为一个全局挂载点 xxx/pv/pvID/globalmount，然后在 NodePublishVolume 接口内为每一个 Pod 去做 bind mount 到 Pod 的挂载点 xxx/pods/podID/mount。

这里利用了 kubelet 的设计机制：在删除 Pod 的多个副本时，只有最后一个 Pod 会调用 NodeUnStageVolume，之前的 Pod 副本不会调用。而社区内很多 CSI 没有使用 NodeStageVolume/NodeUnStageVolume(比如 juicefs-csi 等)，而是在 NodePublishVolume/NodeUnPublishVolume 里去做挂载点判断，这导致 NodePublishVolume/NodeUnPublishVolume 代码逻辑很复杂，而我的实现充分利用了 kubelet CSI 设计机制使得代码逻辑简洁很多。

难点：由于 fuse-client 在 CSI Pod 内启动形成的紧耦合，如果 CSI Pod 由于升级导致的重建，导致业务 Pod 的挂载点损坏，业务 Pod 基本不可用的严重副作用，只有业务 Pod 重新滚动才能解决此类问题。社区内类似这种 fusefs csi 都有这种问题，而大多数 CSI 并没有很好的解决此类问题，只能寄希望于 CSI Pod 永不重建。

但是，我的解决方案完美解决了社区 fuse 分布式文件系统共有难题：CSI Pod 重启，业务 Pod 挂载点损坏，导致业务 Pod 无法读写数据。解决方案为两种：

(1)CSI Pod 重启后，需要加上 fuse recovery 机制，根据 K8S VolumeAttachment 对象获取所有相关数据(VolumeAttachment 由 AttachDetachController 创建)，再在 CSI Pod 内重启 fusefs-client 进程(该方案已经回馈给京东 cubefs-csi 社区)。等 CSI Pod 重建完成，业务 Pod 可以继续读写数据而不用重建，而在该方案之前，却是业务 Pod 一直不可用只能重启才可以(生产环境不现实)，新方案大大减缓了业务 Pod 使用 PVC 的痛点。但是该方案有些瑕疵，在 CSI Pod 重建的几十秒时间内，挂载点依然是坏的，业务 Pod 依然不可读写 PVC 数据，尽管只有几十秒时间不可用，远比之前的一直不可用已经大大进步。

(2)使 fuse-client 进程完全独立于 CSI Pod，单独作为 Fuse Pod 运行实现解耦，CSI Pod 作为控制平面来 create/delete Fuse Pod，并结合 CSI NodeStageVolume/NodeUnstageVolume 和 NodePublishVolume/NodeUnpublishVolume 机制实现代码简洁化。同理，在 NodeStageVolume/NodeUnstageVolume 接口内去 create/delete fuse Deployment，在 NodePublishVolume/NodeUnpublishVolume 去做 bind mount 全局挂载点到每一个 Pod 的挂载点。(已经向 cubefs-csi 和 juicefs-csi 社区反馈)

后续：由于 fusefs 一些性能问题，之前独立调研了 eBPF 在 linux 磁盘 I/O 上的性能优化，包括把 fuse 用户态的一些接口对应的业务逻辑下沉到内核层去执行，无需再从内核态到用户态的一次数据复制，从而达到更高性能。调研发现社区内已经有相关的尝试和文档，不过还不成熟，且貌似还需要修改 linux 文件系统相关代码，重新编译 linux 内核(存疑)。总之，由于时间精力有限，无法继续深入调研。

相关经验：对 K8S CSI 内部机制和源码都比较了解，包括 kubelet 中的 CSI volume 机制以及相关 sidecar 容器内部机制，对 StorageClass 的 Immediate 立即绑定和 WaitForFirstConsumer 延迟绑定 PV 的内部机制都比较了解，总之对 CSI 相关开发经验比较了解。了解 raft 共识算法内部机制，包括 raft leader 选举和日志复制等步骤。和对 linux vfs fuse 机制，分布式文件存储内部机制都有所了解。

现状：fusefs 和 fusefs-csi 目前是公司内部容器云默认的分布式文件存储系统，已经在所有生产 K8S 上部署支持。

### (7) log-operator 以及 filebeat processor 二次开发
开发人员：独立开发整个项目。

背景：需要落地公司业务 Pod 日志到 kafka，并且支持标准输出和本地文件两种模式。所以，我们使用 filebeat 作为日志收集器，并发到 kafka。

(1)开发 filebeat processor。基于公司内搜索部门的特殊需求，开发一个 filebeat processor 插件，在 filebeat 输出的 json log 里加了一些特定的字段，并且需要 filebeat 开启 add_kubernetes_metadata processor。并且把自定义开发的 processor 编译进 filebeat 源码内作为公司内部维护版本，filebeat 没有优雅的 processor 扩展机制，这也是无奈之举。

(2)日志标准输出模式：业务 Pod 把日志打到标准输出，被容器运行时 docker json-file log 模块落盘或者 containerd(kubelet 会负责落盘)。而 filebeat 以 daemonset 形式部署，同时该 daemonset 有我们自研的 sidecar 容器 log-operator，log-operator 支持 docker 和 containerd 两种容器运行时。该 log-operator 会去 watch Pod，并根据 Pod annotation 的配置渲染成 filebeat input log 路径配置，且 Pod annotation 配置是容器级别的 container-level，每一个 container-level 配置包含 kafka 地址和 topic 等一些重要配置。同时，为了高性能需要把一段时间内一组 pods 批处理，而不是每一个 Pod 立刻渲染 filebeat inputs.yml 文件。本模式利用了 filebeat 动态加载配置文件的主要功能，filebeat 会定期检查 inputs.yml，一旦 inputs.yml 增加或删除一个 log input，filebeat 进程会使用最新的 inputs.yml，对于新的一个 log input 就增加一个新的 harvest goroutine 去消费新日志。

后续调研另一种方案：目前方案是 log-operator 作为 sidecar 容器和 filebeat 共享相同的 volumeMount 作为 daemonset 部署在每一个节点，然后 watch pods 去更新 filebeat inputs.yml 文件。还有种资源消耗比较少的方案，就是 log-operator 作为 deployment 部署，filebeat 作为 daemonset 部署，log-operator 会给每一个 node 上 filebeat 容器生成对应的 inputs.yml configmap，名为 filebeat-inputs-{nodename}，利用 configmap volume 热更新原理，只要 configmap 内容变化，作为 volume mount 到容器里的 inputs.yml 就会变化，这时 filebeat config live-reload 功能就会使用最新的配置。该方案利用了 configmap 作为 volume 的热更新技术。该方案的优点是和 filebeat 容器解耦，消耗资源更少，缺点是开发过程稍稍复杂点。

现状：目前由于精力有限，生产 K8S 使用第一种方案，第二种方案已开发完整 demo，只作为测试使用。

(3)日志本地文件模式：有些业务 Pod 由于特殊原因必须把日志打到容器内指定路径，尽管这些业务比较少。开发一个 MutationWebhook 去拦截 Pod 创建，并根据总开关是否需要收集 Pod 日志。对于此类特殊业务 Pod，根据 Pod annotation 配置计算出每一个业务容器的日志输出路径，然后依次挂载在一个 emptyDir volume 内，且同时添加一个 filebeat sidecar 容器，该容器也会挂载这个 emptyDir volume，最后根据这些路径渲染出 filebeat inputs.yml。这样 filebeat 就可以收集每个容器的日志到 kafka。同时，为了减缓 Pod 被 kill 后 filebeat 容器还没收集完日志导致日志丢失，在 filebeat container PreStop lifecycle 内加了 sleep 30s，留足足够的时间去消费日志。

### (8) 废弃的项目
在调研和开发后期，由于不可抗力因素导致的废弃项目：
* etcd-operator etcd 中间件上 K8S 项目：对 etcd 机制和内部原理比较了解，包括对 raft 和 mvcc 的了解，也给组内人员分享过 etcd 内部机制。
* 大数据上 K8S 项目：涉及到的技术栈 spark-operator/flink-operator 和 volcano，对 spark-operator 源码比较熟悉，对 volcano 源码有所了解。

# 个人描述
在项目开发中，顺便对社区相关开源项目做过一些 PR，比如，kube-router、flannel、koordinator、cni plugins、openelb、metallb、cilium、kube-ovn、terway、gobgp、etcd、k8s descheduler、calico，不过都是一些小 PR 小修复。对 K8S kubelet、scheduler、kube-controller-manager 和 kube-proxy 等主要组件源码比较了解，对 cilium、calico 等常见的 CNI 代码有些了解，对 CSI 机制和相关源码比较了解，对 etcd 常见的 kvstore 也比较了解。

对云原生感兴趣，主要关注 kubernetes 生态相关技术，包括但不限于网络、分布式存储、监控、日志和网关相关技术，以及对 eBPF 应用于容器网络和可观测性比较感兴趣，K8S 分布式文件存储 CSI，和对 K8S 调度器方向也比较感兴趣。最后，对混合部署感兴趣，希望有精力可以继续深入这个方向，包括对大数据上 K8S 感兴趣。

做事情认真负责，喜欢团队合作，比较讨厌单打独斗除非没办法，相信团队的力量是无穷的，能做更大的事情。
