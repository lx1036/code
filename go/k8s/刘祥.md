# 个人介绍
* 姓名：刘祥
* 性别：男
* 出生日期：1991-10-10
* 学位：北京理工大学学士(2008-09 ~ 2012-07)/北京航空航天大学硕士(2012-09 ~ 2015-07)
* 工作经验：7 年
* 毕业时间：2015-07-01
* 联系电话/微信号：13426116367
* 电子邮箱：lx1036@126.com或lx20081036@gmail.com
* 技术专栏：https://juejin.cn/user/800100194726088/posts
* 应聘职位：云原生研发工程师(Kubernetes方向)
* K8S证书：CKA 证书

# 工作经历及项目经验

## 北京当当网信息技术有限公司(2015-07 ~ 2016-07)
主要使用PHP语言重构一些老业务代码和迭代业务新功能，主要工作内容包括：
(1)负责当当图书和店铺域的改版和优化，并负责后续版本迭代工作。
(2)负责当当优品馆全面改版项目，对一些老代码进行了重构优化，提高代码可读性。

## RightCapital(2016-07 ~ 2019-07)
参与创业，加入时公司共5个人，北京和纽约办公室各2-3个人。作为全栈工程师参与创业，写后端和前端业务。
主要是用 PHP 语言和 Laravel Web 框架做一款金融软件，面向美国市场。主要工作内容包括：
(1)使用 PHP 框架 Laravel 编写金融软件 RightCapital 后端的 Restful API，并使用 PHPUnit/Mockery 编写单元测试和集成测试。
同时，结合业务需求，对 Laravel 做了很多二次开发，并做成共享私有包，并编写 API 的 Swagger 文档。

(2)使用 Angular 作为前端，Laravel 作为后端，并使用 Ant Design 组件库编写 Admin 后台，供美国客服团队使用。
重写金融软件 RightCapital 前端模块，把其从 Angular.js 重写升级到 Angular 框架。

(3)运维云服务器AWS，搭建一些DevOps软件工具，如Gitlab CI/CD、编写Docker images等等，并使用Terraform/Ansible开发一些提高工作效率的工具等等。

## 奇虎360(2019-08 ~ 至今)
在360搜索部门主要负责一些业务维护和搜索部门k8s云平台维护，主要工作内容包括：
(1)主要维护 360 自研的管理 K8S 的 PASS 平台 UI 工具，也是业务容器发布平台，
维护前端代码和后端代码，技术栈使用前端 typescript 语言的 Angular 框架、后端 golang 的 gin 框架。

(2)参与自研 LoadBalancer Operator，使得 lvs vip 直连 pod ip，主要开发一个controller部署在k8s集群内，并定义crd，业务接入只需要写cr就可以。

(3)二次开发k8s写一些小工具，包括监控oom pod并告警、cronjob定期扫描坏机器并告警，这些小工具直接使用client-go包裸写，没用kubebuilder脚手架。
负责harbor镜像存储平台维护，包括harbor-to-harbor镜像迁移，切换镜像后端存储为hulk技术中台内部s3存储等等杂活，二次开发并接入harbor webhook来统计镜像pull/push记录等等。
统计k8s云平台资源报表，主要使用golang脚本调用k8s api和prometheus api来获取资源数据并存储数据到mysql，最后在grafana上展示整个云平台资源报表，包括cpu、内存、网络和各个业务资源使用情况。
自研CI/CD golang 脚本，主要结合gitlab CI/CD，推送docker image完成后，调用自研的golang脚本，脚本内会调用wayne api来自动化部署镜像，同时wayne页面上也会看到部署记录。
写了一个k8s pod日志CLI工具，使用golang语言写个工具直接读取kafka日志，方便团队直接本地查看日志，提高工作效率。

(4)负责360搜索k8s私有云平台维护，目前只有两个开发和一个运维在负责，我是主要负责人之一。
负责内容包括各个业务稳定性维护和问题排查、calico网络问题排查、ceph分布式存储搭建和维护扩容、kong网关维护、etcd节点问题排查、filebeat日志问题排查等等，承包云平台所有杂活。
正在开始搭建rancher作为devops人员来管理kubernetes的平台，并逐渐替换现有部署kubernetes的方式，由ansible playbook改为rancher rke。
为了更好辅助解决搜索部门成员碰到的各种云平台问题，负责写云原生相关技术内部wiki，以及操作手册wiki。
同时还负责一些垂直搜索业务的维护，主要使用golang语言。



# 容器网络项目
(1) 负载均衡项目LoadBalancer Operator，实现K8s集群内Pod为集群外提供服务
内容：替换旧的通过externalIP service对外暴露服务方式，使用K8s Pod直连公司已有的生产LVS集群方案，实现集群外部流量由LVS集群接管。集群内部负载均衡由
Cilium eBPF 替换 Kube-proxy 来接管，外部由 Cilium+BGP(Bird) 来宣告 Pod Cidr。
LVS VIP直连pod ip，并结合BGP宣告分配给每一个Node的pod cidr使得pod ip公司内网可达，实现LVS集群直接跳转pod所在的node ip，减少网络跳转，提高网络性能。
后续又使用CRD模式重构了一版本并上线，使得配置更简单，且更具有可观测性。
难点：动态感知pod ip变化并更新lvs vip下的rs变化，使得LVS侧RS列表和K8s侧pod列表一致；使用K8s ReadinessGate/Webhook功能使得pod滚动时只有LVS侧资源创建完成后，
该新pod才能才能加入LVS侧RS列表，才能对外提供服务；使用LoadBalancer Webhook实现pod graceful-shutdown，实现pod资源销毁时，流量丢失问题。

(2) 负载均衡项目 LoadBalancer Service/Pod Cidr 的 BGP Speaker，实现暴露 K8S 集群内 Pod 为集群外提供服务
内容：替换外部依赖 LVS 集群方案，借助 K8s 集群内负载均衡也在集群外可达，开发 LoadBalancer Service BGP Speaker 宣告 Service IP，使得 LoadBalancer Service IP 在
集群外可达，可以不再依赖外部 LVS 集群。
同时开发 Pod Cidr BGP Speaker 来宣告 Cilium Operator 给每一个 Node 分配的 Pod Cidr，使得 Pod IP 在集群外可达，并替换 Bird 软件。
目前已经在部分生产 K8S 集群上线。
Daemonset 部署(BGP PodCidr/Service[ClusterIP,ExternalIP,Ingress] Speaker) + Deployment 部署(LoadBalancer Service IPAM)

(3) Cilium IPAM 二次开发项目
内容：针对集群内需要根据 nodeSelector 配置不同的 IPPool 需求，并且尽可能需要一个 Node 可以配置多个 PodCIDR，实现当 Node 的 IP 资源不足时，
可以动态扩容 IP。该需求类似于 Calico 支持多个 IPPool 功能，且公司业务需要该功能。
但是 Cilium 目前不内置支持，需要根据 Cilium IPAM 自定义开发。目前已经根据两种不同的 Cilium IPAM 分别开发了对应的 Operator。
* 根据 Cilium Kubernetes IPAM 机制开发自定义的 Operator，结合 cilium-agent 从 K8s Node 和 node annotation 中读取 PodCIDR 机制，选择
关闭 kube-controller-manager 给 K8s Node 分配 PodCIDR，开发自定义 Operator 来根据 node label 选择不同的 IPPool 并分配对应的 PodCIDR，并添加到
node annotation 中，实现了不同 K8s Node 可以选择不同的 IPPool 需求。已经生产已用，符合业务需要，且不需要更改 cilium-agent 以及 BGP 相关的配置参数。
* 更进一步，第一种方案尽管已经满足业务需要，但是该方案不支持一个 Node 配置多个 PodCIDR，所以更进一步，选择 Cilium CRD IPAM 机制，再次开发自定义的 Operator，
支持按需动态扩容和回收节点的 PodCIDR IP 资源。该方案实现复杂，同时需要更改 cilium-agent 相关参数，比如 cilium-agent 默认只会为有且仅有一个 PodCidr 创建一个路由指向
网卡 cilium_host，为了支持多个 PodCIDR，需要开启每一个 Pod 一个路由的配置参数，而出于性能考虑这不是 Cilium 的默认行为，等等几个其他参数配置；
同时还需要修改 BGP speaker 软件 bird 的配置。总之，该方案有点 hack。总之，出于稳定性考虑以及业务需要考虑，暂时在测试 K8s 集群部署使用。


(4) 自定义容器网络插件 CNI 开发项目
内容：针对容器上虚机场景，使用 ipvlan 技术连通不同 namespace 的容器，开发 CNI。
同时使用 iptables/ipset/conntrack 技术基本实现了 NetworkPolicy 功能，包括：。


(5) fusefs-csi 和 fusefs 项目
(5.1)开发 fusefs 项目：fusefs 项目包含 fuse-client、master-cluster(raft) 和 meta-partition-cluster(multi-raft) 三个模块。
部署时 master-cluster 作为控制平面一般部署3节点，使用raft保证数据强一致性；meta-partition-cluster 节点可以无限扩展，每个 meta partition 默认3节点，使用multi-raft保证数据强一致性；
fuse-client 进程置于 fusefs-csi pod 里，每个 k8s worker node 上每个 pv(可以被多个 pod 挂载) 启动一个 fuse-client 进程来实现 fuse 挂载。 

* 主要参与开发 fuse-client 模块，使用 fuse 实现本地化直接读写文件，数据存储在远程 S3 上。inode 和 dentry 数据结构都缓存在本地内存中，
其中 inode 使用 LRU 数据结构存储，根据其有效时间来从 meta-partition-cluster 刷新 inode。并且修改第三方 fuse 包，使其支持 macOS 系统，使得可以 mac 本地运行。

* 主要参与开发 master-cluster 模块，主要提供 meta node 注册相关 api；create/delete volume 等相关 api 被 fusefs-csi 调用，
并在 volume 创建时根据 meta node 使用率选择对应数量的 meta node，在每一个 meta node 上创建包含 inode 范围的 meta partition 数据。
使用 raft 来实现数据强一致性，raft log 和相关 term 等配置数据，存储在 boltdb 中。通过定期 raft snapshot 实现状态机的快照，
状态机数据比如 volume 及其相关的 meta partition 等数据，也是持久化到 boltdb 中。所以，有两个 boltdb 文件，一个持久化 raft log 和 raft 配置数据，一个是持久化状态机数据。

* 部分参与 meta-partition-cluster 模块，该模块主要管理 inode/dentry 元数据 。每一组 meta partition 由 raft 保证强一致性，多组 meta partition 组成
raft group。对于每一组 raft meta partition，其状态机由两个 B-Tree 分别创建和删除 inode/dentry。

(5.2)开发 fusefs-csi 项目：为团队自研的分布式文件存储实现一个 K8s CSI，方便业务 pod 动态挂载 fusefs pvc。
* 独立开发整个 CSI 项目，完美解决了社区 fuse 分布式文件系统共有难题：CSI Pod 重启，业务 Pod 挂载点损坏，导致业务 Pod 无法读写数据。解决方案为两期：
  * CSI Pod 重启后，需要加上 fuse recovery 机制，根据 K8s VolumeAttachment 对象获取所有相关数据，再在 CSI Pod 内重启 fusefs-client 进程。(已经回馈给京东 cubefs-csi 社区)
  * 使得 fusefs-client 完全独立于 CSI Pod，单独作为 Fuse Pod 运行，CSI Pod 作为控制平面来 create/delete Fuse Pod，并结合 CSI NodeStageVolume/NodeUnstageVolume 和 NodePublishVolume/NodeUnpublishVolume 机制实现代码简洁化，相比于 juicefs-csi 实现逻辑。(已经向 juicefs-csi 社区反馈)

后续：独立调研 eBPF 在 linux 磁盘 I/O 上的性能优化，包括把 fuse 用户态的一些接口对应的业务逻辑下沉到内核层去执行，无需再从内核态到用户态的一次数据复制，从而达到更高性能。
调研发现社区内已经有相关的尝试和文档，不过还不成熟，且貌似还需要修改 linux 文件系统相关代码，重新编译 linux 内核(存疑)。总之，由于时间精力有限，无法继续深入调研。

熟悉 raft 共识算法，熟悉 linux vfs fuse 机制和 linux 磁盘读写 I/O 机制，熟悉分布式文件存储内部机制

etcd-operator 开发：
对 etcd 机制和内部原理比较了解

(6) log-operator 以及 filebeat processor 二次开发
基于公司内搜索部门的特殊需求，二次开发一个 filebeat processor 插件，并编译进 filebeat 源码内作为公司内部维护版本。
同时为支持

并对 filebeat 源码和运行机制有一定了解。



(7) 在离线混合部署项目开发
(7.1) 基于阿里混部内核的 POC 验证测试

(7.2) pod 调度器 plugin 开发
对


相关经验：对容器网络、存储和调度有不错的了解和实践，对在离线混合部署和大数据上K8s有一定的实践经验。


# 个人描述
做事情认真负责，喜欢团队合作。
对云原生感兴趣，主要关注 kubernetes 生态相关技术，包括但不限于网络、分布式存储、监控、日志和网关相关技术。对 eBPF 应用于容器网络和可观测性比较感兴趣。





# 面试

## 数据结构和算法
(1)fusefs-client 中的 LRU(哈希表和双向链表)
(2)fusefs meta-node 中的 b-tree，以及 b+tree
(3)优先级队列(平衡树、最小堆、最大堆)
(4)单向链表和双向链表
(5)二叉树
(6)LRU数据结构
